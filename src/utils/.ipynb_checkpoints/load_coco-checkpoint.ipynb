{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2861444e-3266-4437-8c9b-1cab4c853b4e",
   "metadata": {},
   "source": [
    "### COCO Dataset Format\n",
    "\n",
    "The COCO (Common Objects in Context) dataset format is widely used for object detection, segmentation, and keypoint detection tasks. The dataset is stored in a JSON file with the following key components:\n",
    "\n",
    "1. **Images**:\n",
    "    - Contains metadata for each image, such as:\n",
    "      - `id`: A unique identifier for the image.\n",
    "      - `file_name`: The name of the image file.\n",
    "      - `height`: The height of the image in pixels.\n",
    "      - `width`: The width of the image in pixels.\n",
    "\n",
    "2. **Annotations**:\n",
    "    - Contains annotations for objects within each image, such as:\n",
    "      - `id`: A unique identifier for the annotation.\n",
    "      - `image_id`: The ID of the image to which this annotation belongs.\n",
    "      - `category_id`: The ID of the category this object belongs to.\n",
    "      - `bbox`: (Optional) The bounding box of the object `[x, y, width, height]`.\n",
    "      - `segmentation`: (Optional) The segmentation mask for the object.\n",
    "      - `area`: (Optional) The area of the object in pixels.\n",
    "      - `iscrowd`: Indicates whether the annotation represents a crowd (1) or a single object (0).\n",
    "\n",
    "3. **Categories**:\n",
    "    - Contains a list of categories for classification, including:\n",
    "      - `id`: A unique identifier for the category.\n",
    "      - `name`: The name of the category (e.g., \"cat\", \"dog\").\n",
    "\n",
    "This format is designed to facilitate various computer vision tasks by organizing images, annotations, and categories in a standardized way, making it easier to train and evaluate models on diverse datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026de0ca-257a-4157-a1f3-c5e19d17c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools import mask as maskUtils\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b4d3fd-26d6-447f-8854-75a48f1de4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to your database\n",
    "conn = sqlite3.connect('dataset.db')\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70479eb-8bf8-4d19-a2de-1d6b73a68cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract unique class names from segmentation annotations\n",
    "cursor.execute(\"SELECT DISTINCT name FROM annotations WHERE type='segmentation'\")\n",
    "unique_class_names = cursor.fetchall()\n",
    "\n",
    "# Step 2: Create the `categories` table\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS categories (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT UNIQUE NOT NULL\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Step 3: Insert unique class names into the `categories` table\n",
    "for class_name in unique_class_names:\n",
    "    cursor.execute(\"INSERT OR IGNORE INTO categories (name) VALUES (?)\", (class_name[0],))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062d67fc-4061-4eea-ad2e-247af29f9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch images\n",
    "cursor.execute(\"SELECT id, filename, height, width FROM images\")\n",
    "images = []\n",
    "for row in cursor.fetchall():\n",
    "    images.append({\n",
    "        \"id\": row[0],\n",
    "        \"file_name\": row[1],\n",
    "        \"height\": row[2],\n",
    "        \"width\": row[3]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3f958c-3771-4b74-83e9-1fe2b3dd492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fetch categories\n",
    "cursor.execute(\"SELECT id, name FROM categories\")\n",
    "categories = []\n",
    "category_id_map = {}  # Map category name to ID for fast lookup\n",
    "for row in cursor.fetchall():\n",
    "    category_id_map[row[1]] = row[0] - 1\n",
    "    categories.append({\n",
    "        \"id\": row[0] - 1,\n",
    "        \"name\": row[1],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2689d959-15b9-4339-a68c-84745298f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fetch annotations\n",
    "cursor.execute(\"SELECT id, name, value, imageID FROM annotations WHERE type='segmentation'\")\n",
    "annotations = []\n",
    "for row in cursor.fetchall():\n",
    "    annotation_id, class_name, mask_path, image_id = row\n",
    "    \n",
    "    # Get the corresponding category ID\n",
    "    category_id = category_id_map[class_name]\n",
    "    \n",
    "    # Fetch the image size from the database or any other source\n",
    "    cursor.execute(\"SELECT width, height FROM images WHERE id=?\", (image_id,))\n",
    "    image_width, image_height = cursor.fetchone()  # Assuming the size is stored in the 'images' table\n",
    "\n",
    "    # Open the mask image and resize it to match the image size\n",
    "    with Image.open('datasets/masks/' + mask_path) as mask:\n",
    "        mask = mask.convert(\"L\")  # Ensure it's a grayscale image\n",
    "        original_size = mask.size  # Save the original size for reference if needed\n",
    "        mask = mask.resize((image_width, image_height), Image.NEAREST)  # Resize the mask to match the image size\n",
    "\n",
    "        mask_array = np.array(mask)\n",
    "        binary_mask = mask_array > 0  # Convert to binary mask\n",
    "\n",
    "        # Find contours (external contours only)\n",
    "        contours, _ = cv2.findContours(binary_mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Convert contours to COCO format (list of lists)\n",
    "        segmentation = []\n",
    "        for contour in contours:\n",
    "            contour = contour.flatten().tolist()  # Flatten the contour array\n",
    "            if len(contour) > 4:  # Valid polygon must have at least 3 points (6 coordinates)\n",
    "                segmentation.append(contour)\n",
    "        \n",
    "        # Calculate the area using the binary mask\n",
    "        area = np.sum(binary_mask)\n",
    "        \n",
    "        # Calculate the bounding box\n",
    "        x, y, w, h = cv2.boundingRect(binary_mask.astype(np.uint8))\n",
    "        bbox = [x, y, w, h]\n",
    "\n",
    "    # Add the annotation to the list\n",
    "    annotations.append({\n",
    "        \"id\": annotation_id,\n",
    "        \"image_id\": image_id,\n",
    "        \"category_id\": category_id,\n",
    "        \"segmentation\": segmentation,  # List of polygon points\n",
    "        \"area\": int(area),\n",
    "        \"iscrowd\": 0,\n",
    "        \"bbox\": [[int(coord) for coord in bbox]],  # Convert to list of integers\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a1f4fa-a249-430d-9f73-a55f54faa76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Assemble the COCO dataset\n",
    "coco_dataset = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28793b55-eadf-4c5e-bf96-60867ee197f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the COCO dataset to a JSON file\n",
    "with open('datasets/coco/coco_dataset.json', 'w') as f:\n",
    "    json.dump(coco_dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25deaa6-ed76-468e-8208-eaed1f4a6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28488f3d-d21c-47c8-bd16-0c92bda59fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_file_modification_time(file_path):\n",
    "    return datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "\n",
    "def combine_folders_and_update_coco(coco_json_path, input_folder, output_folder):\n",
    "    # Load the COCO dataset\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    images = coco_data['images']\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    seen_files = {}\n",
    "\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff')):\n",
    "                src_file_path = os.path.join(root, file)\n",
    "                dst_file_path = os.path.join(output_folder, file)\n",
    "\n",
    "                # Check if the file already exists in the destination folder\n",
    "                if file in seen_files:\n",
    "                    existing_file_path = seen_files[file]\n",
    "                    # Compare modification times and keep the more recent one\n",
    "                    if get_file_modification_time(src_file_path) > get_file_modification_time(existing_file_path):\n",
    "                        os.remove(dst_file_path)\n",
    "                        shutil.copy2(src_file_path, dst_file_path)\n",
    "                        seen_files[file] = src_file_path\n",
    "                        print(f\"Replaced {existing_file_path} with more recent {src_file_path}\")\n",
    "                    else:\n",
    "                        print(f\"Skipped {src_file_path} as {existing_file_path} is more recent\")\n",
    "                else:\n",
    "                    shutil.copy2(src_file_path, dst_file_path)\n",
    "                    seen_files[file] = src_file_path\n",
    "                    print(f\"Copied {src_file_path} to {dst_file_path}\")\n",
    "\n",
    "    # Update the file_name field in the COCO JSON\n",
    "    for image in images:\n",
    "        image_file_name = os.path.basename(image['file_name'])\n",
    "        if image_file_name in seen_files:\n",
    "            image['file_name'] = os.path.join('images', image_file_name)\n",
    "\n",
    "    # Save the updated COCO JSON file\n",
    "    updated_coco_json_path = os.path.join(output_folder, 'updated_coco_dataset.json')\n",
    "    with open(updated_coco_json_path, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "    print(f\"COCO dataset updated and saved to {updated_coco_json_path}\")\n",
    "\n",
    "# Example usage\n",
    "coco_json_path = 'D:/UoL/Final Project/src/datasets/coco/coco_dataset.json'\n",
    "input_folder = \"D:/UoL/Final Project/src/datasets/IOP's\"\n",
    "output_folder = 'D:/UoL/Final Project/src/datasets/images'\n",
    "combine_folders_and_update_coco(coco_json_path, input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e17fa27-081b-41ad-9495-7dfa6086a248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35bc4f4d-eee0-4b2c-8109-f7a9e489fa2e",
   "metadata": {},
   "source": [
    "## create train test and validation splits for yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b085bcc3-b9be-4a00-9301-6809a6c8596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found in the dataset:\n",
      "0: Bone Loss\n",
      "1: Restorations\n",
      "2: Periapical Abnormality\n",
      "3: Fracture\n",
      "4: Implants\n",
      "5: Prosthetics\n",
      "6: Dental Caries\n",
      "7: Absent Tooth\n",
      "8: Impacted Tooth\n",
      "9: other\n",
      "10: Position\n",
      "11: Root Stump\n",
      "12: Altered Morphology\n",
      "13: Attrition\n",
      "14: Microdontia\n"
     ]
    }
   ],
   "source": [
    "# Path to your COCO JSON file\n",
    "coco_json_path = 'D:/UoL/Final Project/src/datasets/updated_coco_dataset.json'\n",
    "\n",
    "# Load the COCO JSON file\n",
    "with open(coco_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Extract class names from the 'categories' field\n",
    "classes = [category['name'] for category in coco_data['categories']]\n",
    "\n",
    "# Print the list of classes\n",
    "print(\"Classes found in the dataset:\")\n",
    "for idx, class_name in enumerate(classes, start=1):\n",
    "    print(f\"{idx-1}: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cec6340-22bd-45c6-8f5a-b78c09fe5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501106ef-6727-47d4-b5db-dae341ebef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coco_to_yolo_segmentation(coco_annotation, image_width, image_height):\n",
    "    # Convert COCO segmentation to YOLO format (normalized coordinates)\n",
    "    yolo_segments = []\n",
    "    for segment in coco_annotation:\n",
    "        yolo_segment = []\n",
    "        for i in range(0, len(segment), 2):\n",
    "            x = segment[i] / image_width\n",
    "            y = segment[i + 1] / image_height\n",
    "            yolo_segment.extend([x, y])\n",
    "        yolo_segments.append(yolo_segment)\n",
    "    return yolo_segments\n",
    "\n",
    "def create_folders_and_txt_files(coco_json_path, output_dir, train_size=0.7, val_size=0.2, test_size=0.1, random_state=42):\n",
    "    # Load the COCO dataset\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    images = coco_data['images']\n",
    "    annotations = coco_data['annotations']\n",
    "    categories = {category['id']: category['name'] for category in coco_data['categories']}\n",
    "    \n",
    "    # Split the dataset into train, val, and test sets\n",
    "    train_images, test_images = train_test_split(images, train_size=train_size, random_state=random_state)\n",
    "    val_images, test_images = train_test_split(test_images, train_size=val_size/(val_size + test_size), random_state=random_state)\n",
    "\n",
    "    # Mapping from image_id to annotations\n",
    "    image_to_annotations = {}\n",
    "    for annotation in annotations:\n",
    "        image_id = annotation['image_id']\n",
    "        if image_id not in image_to_annotations:\n",
    "            image_to_annotations[image_id] = []\n",
    "        image_to_annotations[image_id].append(annotation)\n",
    "    \n",
    "    # Create directories and process images/labels\n",
    "    for split_name, split_images in [('train', train_images), ('valid', val_images), ('test', test_images)]:\n",
    "        image_dir = os.path.join(output_dir, split_name, 'images')\n",
    "        label_dir = os.path.join(output_dir, split_name, 'labels')\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "        for image in split_images:\n",
    "            image_id = image['id']\n",
    "            file_name = image['file_name']\n",
    "            src_image_path = os.path.join(os.path.dirname(coco_json_path), file_name)\n",
    "            dst_image_path = os.path.join(image_dir, os.path.basename(file_name))\n",
    "\n",
    "            # Copy the image to the relevant directory\n",
    "            if os.path.exists(src_image_path):\n",
    "                shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "                # Create the corresponding label file\n",
    "                label_file_path = os.path.join(label_dir, os.path.splitext(os.path.basename(file_name))[0] + '.txt')\n",
    "                with open(label_file_path, 'w') as label_file:\n",
    "                    image_annotations = image_to_annotations.get(image_id, [])\n",
    "                    for annotation in image_annotations:\n",
    "                        if 'segmentation' in annotation:\n",
    "                            segments = coco_to_yolo_segmentation(annotation['segmentation'], image['width'], image['height'])\n",
    "                            category_id = annotation['category_id']\n",
    "                            class_name = categories[category_id]\n",
    "                            class_index = list(categories.values()).index(class_name)\n",
    "                            \n",
    "                            for segment in segments:\n",
    "                                label_file.write(f\"{class_index} {' '.join(map(str, segment))}\\n\")\n",
    "            else:\n",
    "                print(f\"Warning: Image {src_image_path} not found. Skipping...\")\n",
    "\n",
    "    print(\"Dataset has been split, folders created, and YOLO-format segmentation labels generated.\")\n",
    "\n",
    "# Example usage\n",
    "output_dir = 'D:/UoL/Final Project/src/datasets/images'\n",
    "coco_json_path = 'D:/UoL/Final Project/src/datasets/updated_coco_dataset.json'\n",
    "create_folders_and_txt_files(coco_json_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83141d0-8150-4460-b3da-578e20393f0f",
   "metadata": {},
   "source": [
    "## create train test and validation splits for coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3d10af-c90e-4598-856c-a22b80cdf668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File D:/UoL/Final Project/src/datasets\\IOP's/lopa_1_jpg_quality/IOPA (22038).jpg not found. Skipping...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def add_bounding_boxes_to_annotations(coco_annotations):\n",
    "    for annotation in coco_annotations:\n",
    "        if 'bbox' not in annotation:\n",
    "            segmentation = annotation.get('segmentation', [])\n",
    "            if segmentation:\n",
    "                min_x = min(segmentation[0][::2])\n",
    "                min_y = min(segmentation[0][1::2])\n",
    "                max_x = max(segmentation[0][::2])\n",
    "                max_y = max(segmentation[0][1::2])\n",
    "                bbox = [min_x, min_y, max_x - min_x, max_y - min_y]\n",
    "                annotation['bbox'] = bbox\n",
    "    return coco_annotations\n",
    "\n",
    "def split_dataset(coco_data, train_ratio=0.7, val_ratio=0.2):\n",
    "    images = coco_data['images']\n",
    "    annotations = coco_data['annotations']\n",
    "\n",
    "    train_images, test_images = train_test_split(images, train_size=train_ratio)\n",
    "    val_images, test_images = train_test_split(test_images, train_size=val_ratio / (1 - train_ratio))\n",
    "\n",
    "    def filter_annotations(image_set):\n",
    "        image_ids = {image['id'] for image in image_set}\n",
    "        return [annotation for annotation in annotations if annotation['image_id'] in image_ids]\n",
    "\n",
    "    train_annotations = filter_annotations(train_images)\n",
    "    val_annotations = filter_annotations(val_images)\n",
    "    test_annotations = filter_annotations(test_images)\n",
    "\n",
    "    return {\n",
    "        'train': {'images': train_images, 'annotations': train_annotations},\n",
    "        'val': {'images': val_images, 'annotations': val_annotations},\n",
    "        'test': {'images': test_images, 'annotations': test_annotations},\n",
    "    }\n",
    "\n",
    "def save_coco_split(coco_split, image_dir, output_dir):\n",
    "    for split_name, data in coco_split.items():\n",
    "        output_path = os.path.join(output_dir, split_name)\n",
    "        os.makedirs(os.path.join(output_path, 'images'), exist_ok=True)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(output_path, f'{split_name}_coco.json'), 'w') as f:\n",
    "            json.dump({\n",
    "                'images': data['images'],\n",
    "                'annotations': data['annotations'],\n",
    "                'categories': coco_data['categories'],\n",
    "            }, f, indent=4)\n",
    "\n",
    "        for image in data['images']:\n",
    "            src_image_path = os.path.join(image_dir, image['file_name'])\n",
    "            dst_image_path = os.path.join(output_path, 'images', os.path.basename(image['file_name']))\n",
    "\n",
    "            # Handle missing files gracefully\n",
    "            if not os.path.exists(src_image_path):\n",
    "                print(f\"Warning: File {src_image_path} not found. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "# Example usage\n",
    "input_json_path = 'D:/UoL/Final Project/src/datasets/coco/updated_coco_dataset.json'\n",
    "image_dir = 'D:/UoL/Final Project/src/datasets'\n",
    "output_dir = 'D:/UoL/Final Project/src/datasets/coco'\n",
    "\n",
    "# Load the COCO JSON file\n",
    "with open(input_json_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Add bounding boxes if missing\n",
    "coco_data['annotations'] = add_bounding_boxes_to_annotations(coco_data['annotations'])\n",
    "\n",
    "# Split the dataset\n",
    "coco_split = split_dataset(coco_data)\n",
    "\n",
    "# Save the split dataset and copy images, with error handling for missing files\n",
    "save_coco_split(coco_split, image_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca8e59-aeb5-4b58-8d3d-e4d5a7d5b146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
